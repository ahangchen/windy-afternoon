# 西瓜书概念
- 括号表示概念出现的其他页码
- 如有兴趣协同整理，请到[issue](https://github.com/ahangchen/windy-afternoon/issues/2)中认领章节
- 公式采用latex编辑，github不能直接渲染，请到[gitbook页](https://ahangchen.gitbooks.io/windy-afternoon/content/ml/melon/)浏览

## 概念列表
## [第一章 绪论](ch01.md)
- Page2: 标记（label)
- Page2: 假设(269)(hypothesis)
- Page2: 示例(instance)
- Page2: 属性(attribute)
- Page2: 属性空间(attribute space)
- Page2: 数据集(data set)
- Page2: 特征(247)(feature)
- Page2: 学习(learning)
- Page2: 学习器(learner)
- Page2: 训练(training)
- Page2: 训练集(training data)
- Page2: 训练样本(training sample)
- Page2: 样本(sample)
- Page2: 样本空间(sample space)
- Page2: 样例(sample)
- Page2: 真相(ground-truth)
- Page3: 标记空间(label space)
- Page3: 测试(testing)
- Page3: 测试样本(testing sample)
- Page3: 簇(197)（cluster）
- Page3: 独立同分布(267)（independent and identically distributed）
- Page3: 多分类（multi-class classification）
- Page3: 二分类（binary classification)
- Page3: 泛化（121，350）（generalization)
- Page3: 分类（classification)
- Page3: 回归（regression）
- Page3: 监督学习（supervised learning）
- Page3: 聚类(197)（clustering）
- Page3: 无导师学习
- Page3: 无监督学习(197)（unsupervised learning）
- Page3: 有导师学习
- Page4: 概念学习(17)（concept learning）
- Page4: 归纳学习(11)（inductive learning）
- Page5: 版本空间（version space）
- Page6: 归纳偏好（inductive bias）
- Page6: 偏好
- Page7: 奥卡姆剃刀(17)（Occam's razor）
- Page10: 符号主义(363)（symbolism）
- Page10: 连接主义（connectionism）
- Page10: 人工智能
- Page11: 机械学习
- Page11: 类比学习
- Page11: 示教学习
- Page12: 统计学习(139)
- Page14: 数据挖掘
- Page16: WEKA
- Page17: 迁移学习

## [第二章 模型评估与选择](ch02.md)
- Page23: 错误率(error rate)
- Page23: 泛化误差（generalization error)
- Page23: 过拟合(104,191,352)（overfitting）
- Page23: 过配
- Page23: 精度(29)(accuracy)
- Page23: 经验误差(267)(empirical error)
- Page23: 欠配（underfitting）
- Page23: 误差(error)
- Page23: 训练误差（trainning error)
- Page24: 模型选择(model selection)
- Page25: 分层采样
- Page25: 留出法
- Page26: k折交叉验证
- Page26: 交叉验证法
- Page27: 包外估计(179)
- Page27: 自助法
- Page28: 参数调节
- Page28: 验证集(105)
- Page29: 均方误差(54)
- Page30: 查全率
- Page30: 查准率
- Page30: 混淆矩阵
- Page30: 召回率
- Page30: 准确率
- Page31: P-R曲线
- Page31: 平衡点
- Page32: F1
- Page32: 宏F1
- Page32: 宏查全率
- Page32: 宏查准率
- Page32: 微F1
- Page32: 微查准率
- Page32: 微查全率
- Page33: ROC曲线(46)
- Page35: 代价(47)
- Page35: 代价矩阵
- Page36: 代价敏感(67)
- Page36: 代价曲线
- Page36: 规范化(183)
- Page36: 归一化
- Page36: 总体代价
- Page37: 假设检验
- Page38: 二项检验
- Page38: 置信度
- Page40: 交叉验证成对t校验
- Page41: 5x2交叉验证
- Page41: McNemar检验
- Page41: 列联表(187)
- Page42: Friedman检验
- Page43: Nemenyi后续检验
- Page44: 偏差-方差分解(177)

## [第3章 线性模型](ch03.md)
- Page53: 线性回归(252)
- Page53: 线性模型
- Page54: 参数学习
- Page54: 平方损失
- Page54: 最小二乘法(72)
- Page55: 多元线性回归
- Page56: 对数线性回归
- Page56: 正则化(105,133)
- Page57: 对数几率回归
- Page57: 广义线性模型
- Page57: 阶跃函数(98)
- Page57: 联系函数
- Page58: Sigmoid函数(98,102)
- Page58: 对率函数
- Page58: 对率回归(132,325)
- Page58: 对数几率函数(98)
- Page58: 几率
- Page58: 替代函数
- Page59: 对数似然(149)
- Page59: 极大似然法(149,297)
- Page60: Fisher判别分析
- Page60: 线性判别分析(139)
- Page61: 广义瑞利商
- Page61: 类间散度矩阵(138)
- Page61: 类内散度矩阵(138)
- Page62: 全局散度矩阵
- Page63: MvM
- Page63: OVO
- Page63: OvR
- Page63: 多分类器学习
- Page64: ECOC
- Page64: 纠错输出码
- Page65: 编码矩阵
- Page66: 类别不平衡(209)
- Page67: 过采样
- Page67: 欠采样
- Page67: 上采样
- Page67: 稀疏表示(255)
- Page67: 稀疏性
- Page67: 下采样
- Page67: 阈值移动
- Page67: 再平衡
- Page67: 再缩放
- Page68: 多标记学习

## [第四章 决策树](ch04.md)
- Page73: 决策树(363)
- Page73: 判定树
- Page74: 分而治之
- Page75: ID3决策树
- Page75: 划分选择
- Page75: 信息增益
- Page77: 增益率
- Page78: C4.5决策树(page83)
- Page79: CART决策树
- Page79: 后剪枝
- Page79: 基尼指数
- Page79: 剪枝(352)
- Page79: 预剪枝(352)
- Page82: 决策树桩
- Page83: 离散化
- Page85: 缺失值
- Page88: 多变量决策树(92)
- Page90: 斜决策树
- Page92: 增量学习(109)

## [第五章 神经网络](ch05.md)
- Page97: M-P神经元模型
- Page97: 人工神经网络
- Page97: 神经网络
- Page97: 神经元
- Page97: 阈值(104)
- Page98: 感知机
- Page98: 激活函数
- Page98: 挤压函数
- Page98: 阈值逻辑单元
- Page99: 非线性可分
- Page99: 功能神经元
- Page99: 收敛
- Page99: 线性超平面
- Page99: 线性可分(126)
- Page99: 学习率
- Page99: 哑结点
- Page99: 振荡
- Page100: 多层前馈神经网络
- Page101: BP算法
- Page101: BP网络
- Page101: 单隐层网络
- Page101: 反向传播算法
- Page101: 连接权(104)
- Page101: 误差逆传播
- Page102: 梯度下降（254，389，407
- Page103: 链式法则(402)
- Page105: 累积误差逆传播
- Page105: 早停
- Page106: 参数空间
- Page106: 局部极小
- Page106: 全剧最小
- Page107: 模拟退火
- Page107: 遗传算法
- Page108: ART网络
- Page108: RBF网络
- Page108: 径向基函数
- Page108: 竞争型学习
- Page108: 胜者通吃
- Page108: 自适应谐振理论
- Page109: Kohonen网络
- Page109: SOM网络
- Page109: 可塑性-稳定性窘境
- Page109: 在线学习(241,393)
- Page109: 自组织映射
- Page110: 级联相关
- Page111: Boltzmann分布
- Page111: Boltzmann机
- Page111: Elman网络
- Page111: 递归神经网络
- Page111: 基于能量的模型
- Page112: 对比散度
- Page112: 受限Boltzmann机
- Page113: 发散
- Page113: 卷及神经网络
- Page113: 权共享
- Page113: 深度学习
- Page113: 无监督逐层训练
- Page114: ReLU
- Page114: 表示学习
- Page114: 汇合
- Page114: 特征学习
- Page115: 广义δ规则
- Page115: 可解释性(191)


## [第6章 支持向量机](ch06.md)
- Page121: 划分超平面
- Page122: 间隔
- Page122: 支持向量
- Page123: SVM
- Page123: 对偶问题(405)
- Page124: KKT条件(124,132,135)
- Page126: 核函数
- Page127: 核技巧
- Page127: 支持向量展式
- Page128: RKHS
- Page128: 高斯核
- Page128: 核矩阵(138,233)
- Page128: 线性核
- Page128: 再生核希尔伯特空间
- Page129: 软间隔
- Page129: 硬间隔
- Page130: 0/1损失函数(page147)
- Page130: hinge损失
- Page130: 对率损失
- Page130: 松弛变量
- Page130: 替代损失
- Page130: 指数损失(173)
- Page131: 软间隔支持向量机
- Page133: 罚函数法
- Page133: 结构风险
- Page133: 经验风险
- Page133: 支持向量回归
- Page137: Mercer定理
- Page137: 表示定理
- Page137: 核方法
- Page137: 核化(232)
- Page137: 核线性判别分析
- Page139: 割平面法
- Page140: 多核学习
- Page140: 一致性

## [第7章 贝叶斯分类器](ch07.md)
- Page147: 贝叶斯风险
- Page147: 贝叶斯最优分类器
- Page147: 风险
- Page147: 条件风险
- Page148: 贝叶斯定理
- Page148: 判别式模型(325)
- Page148: 生成式模型(295,325)
- Page148: 似然
- Page148: 先验
- Page148: 证据
- Page149: 极大似然估计
- Page150: 朴素贝叶斯分类器
- Page150: 条件独立性假设(305)
- Page153: 拉普拉斯修正
- Page154: 半监督贝叶斯分类器
- Page154: 独依赖估计
- Page154: 懒惰学习(225,240)
- Page155: 超父
- Page156: 贝叶斯网(319,339)
- Page156: 概率图模型(319)
- Page156: 信念网
- Page158: V型结构
- Page158: 边际独立性
- Page158: 边际化(328)
- Page158: 道德图
- Page158: 端正图
- Page158: 同父
- Page158: 有向分离
- Page159: 最小描述长度
- Page161: 吉布斯采样(334)
- Page161: 近似推断(161)
- Page161: 精确推断(328,331)
- Page161: 马尔科夫链
- Page161: 平稳分布
- Page162: EM算法(208,295,335)
- Page162: 隐变量(319)
- Page163: 边际似然
- Page163: 坐标下降(408)
- Page164: 贝叶斯分类器
- Page164: 贝叶斯学习

## [第8章 集成学习](ch08.md)
- Page171: 多分类器系统
- Page171: 个体学习器
- Page171: 基学习器
- Page171: 基学习算法
- Page171: 集成学习(311)
- Page171: 弱学习器
- Page172: AdaBoost
- Page172: 多样性
- Page172: 投票法(225)
- Page173: Boosting(page139)
- Page173: 加性模型
- Page177: 重采样
- Page177: 重赋权
- Page178: Bagging
- Page178: 自助采样法
- Page179: 随机森林
- Page182: 加权平均(225)
- Page182: 简单平均
- Page182: 绝对多数投票
- Page183: 加权投票(225)
- Page183: 相对多数投票
- Page184: Stacking
- Page185: 贝叶斯模型平均
- Page185: 分歧(304)
- Page185: 误差-分歧分解
- Page187: 差异性度量
- Page187: 多样性度量
- Page189: 属性子集
- Page189: 随机子空间
- Page189: 稳定基学习器
- Page189: 子空间(227)
- Page191: 混合专家
- Page191: 集成修剪
- Page191: 选择性集成
- Page192: Hoeffding不等式(268)

## [第9章 聚类](ch09.md)
- Page197: 有效性指标
- Page199: 距离度量
- Page200: 街区距离
- Page200: 离散属性
- Page200: 连续属性
- Page200: 列名属性
- Page200: 曼哈顿距离
- Page200: 闵可夫斯基距离(220)
- Page200: 欧氏距离
- Page200: 切比雪夫距离
- Page200: 数值属性
- Page200: 无序属性
- Page200: 有序属性
- Page201: 非度量距离
- Page201: 混合属性
- Page201: 加权距离
- Page201: 距离度量学习(237)
- Page201: 相似度度量
- Page202: k均值算法(218)
- Page202: 原型聚类
- Page204: LVQ(218)
- Page204: 学习向量化
- Page206: 概率模型(319)
- Page206: 高斯混合(296)
- Page211: 密度聚类
- Page214: 层次聚类
- Page219: 聚类集成
- Page219: 异常检测
- Page220: 豪斯多夫距离

## [第10章 降维与度量学习](ch10.md)
- Page225: k近邻
- Page225: 急切学习
- Page225: 平均法
- Page225: 最近邻分类器
- Page226: 密采样
- Page227: 多维缩放
- Page227: 降维
- Page227: 维数约简
- Page227: 维数灾难(247)
- Page229: PCA
- Page229: 线性降维
- Page229: 主成分分析
- Page231: 奇异值分解(402)
- Page232: 本真低维空间
- Page232: 非线性降维
- Page232: 核化线性降维
- Page232: 核主成分分析
- Page234: 本真距离
- Page234: 测地线距离
- Page234: 等度量映射
- Page234: 流形学习
- Page235: 局部线性嵌入
- Page237: 度量学习
- Page238: 近邻成分分析
- Page239: 必连约束(307)
- Page239: 勿连约束
- Page240: 半监督聚类(307)
- Page240: 多视图学习
- Page240: 流形假设(294)
- Page240: 流形正则化

## [第11章 特征选择与稀疏学习](ch11.md)
- Page247: 冗余特征
- Page247: 数据预处理
- Page247: 特征选择
- Page247: 相关特征
- Page248: 子集搜索
- Page248: 子集评价
- Page249: 过滤式特征选择
- Page250: 包裹式特征选择
- Page251: 拉斯维加斯方法
- Page251: 蒙特卡洛方法(340,384)
- Page252: LASSO(261)
- Page252: Tikhonov正则化
- Page252: 岭回归
- Page252: 嵌入式特征选择
- Page253: L1正则化
- Page253: L2正则化
- Page253: Lipschitz条件
- Page253: 近端梯度下降(259)
- Page255: 码书学习
- Page255: 稀疏编码
- Page255: 字典学习
- Page257: 压缩感知
- Page259: 局部线性嵌入
- Page259: 协调过滤
- Page260: 核范数
- Page260: 迹范数

## [第12章 计算学习理论](ch12.md)
- Page267: 计算学习理论
- Page268: Jensen不等式
- Page268: McDiarmid不等式
- Page268: 概率近似正确
- Page268: 概念类
- Page268: 假设空间
- Page269: PAC辨识
- Page269: PAC可学习
- Page269: 不可分(272)
- Page269: 不一致
- Page269: 可分(270)
- Page269: 时间复杂度
- Page270: PAC学习算法
- Page270: 恰PAC可学习
- Page270: 样本复杂度
- Page270: 有限假设空间
- Page273: VC维(274)
- Page273: 不可知PAC可学习
- Page273: 打散
- Page273: 对分
- Page273: 增长函数
- Page278: 经验风险最小化
- Page279: Rademacher复杂度
- Page284: 稳定性
- Page285: 均匀稳定性

## [第13章 半监督学习](ch13.md)
- Page293: 半监督学习(294)
- Page293: 查询
- Page293: 未标记样本
- Page293: 有标记样本
- Page293: 主动学习
- Page294: 聚类假设
- Page295: 直推学习
- Page298: S3VM
- Page298: 半监督SVM
- Page298: 低维嵌入
- Page300: 图半监督学习
- Page301: 亲和矩阵
- Page302: 标记传播
- Page304: 基于分歧的方法
- Page304: 协同训练

## [第14章 概率图模型](ch14.md)
- Page319: 马尔科夫网
- Page319: 推断
- Page319: 隐马尔科夫模型
- Page322: 马尔科夫随机场
- Page322: 视图
- Page322: 因子
- Page323: 全局马尔科夫性
- Page324: 局部马尔科夫性
- Page325: 成对马尔科夫性
- Page325: 马尔科夫毯
- Page325: 条件随机场
- Page328: 边际分布
- Page328: 变量消去
- Page330: 信念传播(340)
- Page331: MCMC
- Page333: MH算法
- Page334: 变分推断
- Page334: 盘式记法
- Page335: KL散度(414)
- Page337: 话题模型
- Page337: 平均场
- Page337: 隐狄利克雷分配模型
- Page340: 非参数化方法

## [第15章 规则学习](ch15.md)
- Page347: 规则
- Page347: 规则学习
- Page347: 逻辑文字
- Page348: 冲突消解
- Page348: 待续规则
- Page348: 命题规则
- Page348: 默认规则
- Page348: 缺省规则
- Page348: 一阶规则
- Page348: 优先级规则
- Page348: 元规则
- Page348: 原子命题
- Page349: 序贯覆盖
- Page350: 特化
- Page352: 似然率
- Page353: RIPPER
- Page357: ILP(364)
- Page357: 归纳逻辑程序设计(364)
- Page358: 最小一般泛化
- Page359: 归纳
- Page359: 逆归结
- Page359: 演绎
- Page361: 合一
- Page361: 置换
- Page361: 最一般合一置换
- Page362: 归结商
- Page363: 关系学习
- Page364: 统计关系学习


## [第16章 强化学习](ch16.md)
- Page371: MDP
- Page371: 奖赏
- Page371: 马尔科夫决策过程
- Page371: 强化学习
- Page371: 再励学习
- Page372: 策略
- Page373: K-摇臂赌博机
- Page374: ϵ-贪心
- Page374: 探索-利用窘境
- Page375: Softmax
- Page377: 有模型学习
- Page377: 状态-动作值函数
- Page377: 状态值函数
- Page380: Bellman等式
- Page381: 策略迭代
- Page382: 免模型学习
- Page382: 值迭代
- Page386: TD学习(393)
- Page386: 时序差分学习(393)
- Page387: Q-学习(393)
- Page387: Sarsa算法(390)
- Page388: 表格值函数
- Page388: 值函数近似
- Page390: 模仿学习
- Page391: 逆强化学习
- Page393: 近似动态规划

## [附录](ch17.md)
- Page399: 行列式
- Page399: 迹
- Page400: Frobenius范数
- Page402: 低秩矩阵近似
- Page403: 拉格朗日乘子法
- Page405: 对偶函数
- Page406: 二次规划
- Page407: 半正定规划
- Page409: 伯努利分布
- Page409: 均匀分布
- Page410: 多项分布
- Page410: 二项分布
- Page411: 贝塔分布
- Page412: 狄利克雷分布
- Page412: 高斯分布
- Page412: 正态分布
- Page413: 共轭分布
- Page414: 相对熵
- Page414: 信息散度
- Page415: 交叉熵
- Page415: 熵
